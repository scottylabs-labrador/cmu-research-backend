"","","Date Added","Project Title","Accepting Applications?","Keywords","Location","Department","Time Commitment (Hours/Week)"
"","Project Contact:
 Arthur Jakobsson - ajakobss@andrew.cmu.edu 
 Project Description:
 We are exploring the efficacy of using pins (weights or physical pins) for aiding manipulation of complex objects. Multiple arms are often useful to perform manipulations but sometimes we do not have access to those resources and therefore we hypothesize that we can imitate extra manipulators by fixing an object with pins.
 Prerequisite Knowledge:
 Machine Learning (10-301 or equivalent), Systems (15-213 or equivalent), higher level ML courses preferred but not required (especially CV and deep learning related), no robotics experience required.
 Link(s) to Relevant Papers: 
 
 Project URLs:","09-09-2025","Robotic Pin Manipulation","Yes","Machine Learning, Manipulation, Deformable objects, TAMP","On Campus","RI","15-20"
"","Project Contact:
 Zheyuan Hu - zheyuanh@andrew.cmu.edu 
 Project Description:
 This project aims to develop a general reasoning agent for bimanual robotic manipulation tasks, especially those that are long-horizon and requires exploration.  Imagine giving the robot a box of assembly toys and it needs to play with them with curiosities and figure out the correct strategy to build them.  Or imagine giving the robot a basket of laundry and it needs to figure out which ones to fold and store and which ones to hang on rack.
 Prerequisite Knowledge:
 Must have: taken linear algebra, able to code in python fluently, not afraid of hands-on work.  Nice to have: experience with LLM, VLM, AI agent, understandings of probability, statistics, machine learning, deep learning, robotics, 3D design, software or infra development experience. Most importantly, willingness to learn and at least 8 hours a week to spend on research.
 Link(s) to Relevant Papers: 
 https://rac-scaling-robot.github.io/
 Project URLs:","09-16-2025","Reasoning for Robotic Manipulation","Yes","VLM, Robot Learning, AI","On Campus","RI","at least 8 hours a week"
"","Project Contact:
 Zheyuan Hu - zheyuanh@andrew.cmu.edu 
 Project Description:
 This project aims to develop novel deep reinforcement learning algorithm and robotic system that can robustly and autonomously learn and improve complex skills through real world interaction.
 Prerequisite Knowledge:
 Must have: taken linear algebra, able to code in python fluently, not afraid of hands-on work.  Nice to have: understandings of probability, statistics, machine learning, deep learning, reinforcement learning, computer vision, robotics, 3D design, software or infra development experience.  Most importantly, willingness to learn and at least 8 hours a week to spend on research.
 Link(s) to Relevant Papers: 
 https://arxiv.org/abs/2509.07953 https://arxiv.org/abs/2401.16013
 Project URLs:","09-16-2025","Autonomous Improvements of Bimanual Dexterous Skills in the Real World","Yes","Robot Learning, Dexterous Manipulation, Imitation Learning, Reinforcement Learning","On Campus","RI","at least 8 hours a week"
"","Project Contact:
 Phillip Compeau - pcompeau@andrew.cmu.edu 
 Project Description:
 Are you interested in seeing SCS have a bigger impact in online education, especially on YouTube?   The Office of Innovation in Computing Education is a new initiative within SCS aimed at increasing the school's impact in open education, with an emphasis on creating and maintaining video content.  Let us know why you're a great fit athttps://forms.gle/Cq9nxNEdDKogQ42K7.
 Prerequisite Knowledge:
 We are looking for students who can help with video editing, as well as students who are interested in web design, visual design, and social media.
 Link(s) to Relevant Papers: 
 
 Project URLs:
 (in progress!)","09-01-2025","Office of Innovation in Computing Education","Yes","Open Education, Video, Websites","On Campus","CBD","flexible"
"","Project Contact:
 Phillip Compeau - pcompeau@andrew.cmu.edu 
 Project Description:
 Programming for Lovers (P4-️) is a free online course that teaches programming  by immersing learners in fun scientific applications. We are expanding the course to include Java and Python as well as have a lot more content, and we are looking for coders, designers, and dreamers to help us grow. We're looking for students with some background in user experience, video editing, or coding to grow the project.  To express interest, please note that the email contact is a blank -- usehttps://forms.gle/heKYSvcJChsQqXzf6 to express interest.
 Prerequisite Knowledge:
 Depends on your task; design, video editing, and coding will have different backgrounds.
 Link(s) to Relevant Papers: 
 
 Project URLs:
 https://programmingforlovers.com","09-01-2025","Programming for Lovers","Yes","Open Education, Programming","On Campus","CBD","flexible"
"","Project Contact:
 George A Kantor - gkantor@andrew.cmu.edu 
 Project Description:
 We are seeking a highly motivated undergraduate student to join our research team to develop an automated scoring system for an interactive mechatronic game in partnership with a local entertainment company. This role offers the opportunity to work at the intersection of computer vision, real-time systems, and mechatronic integration. Responsibilities include design and implement computer vision pipelines for real-time detection and segmentation of game elements, and integrating outputs of the vision system with an existing scoring app. There will also be possibilities to get into the design of electromechanical game elements.
 Prerequisite Knowledge:
 Required Qualifications Experience with computer vision detection and image segmentation techniques. Proficiency in implementing and deploying real-time vision software (e.g., OpenCV, PyTorch, TensorFlow, or similar). Strong programming skills in Python or C++.  Desired Qualifications Hands-on experience with mechatronic systems, particularly integrating vision with hardware. Familiarity with microcontrollers (Arduino, STM32, etc.) or PLCs. Experience developing web app interfaces and integrating real-time communication using WebSocket, TLS, and MQTT Prior experience working on interactive or embedded systems projects.
 Link(s) to Relevant Papers: 
 
 Project URLs:","08-28-2025","Computer Vision for Arcade Game","Yes","computer vision, electromechanical integration","On Campus","RI","10"
"","Project Contact:
 Chenyan Xiong - cx@andrew.cmu.edu 
 Project Description:
 Can we expand foundation models into multiple specialized domains without losing their original capabilities? This project focuses on advancing Mixture-of-Experts (MoE) models to seamlessly integrate new verticals such as healthcare, finance, or scientific text while preserving core language abilities. We aim to: Adapt MoE architectures by adding new domain-specific experts for verticals like EHR or financial text. Explore mid-training and continual pretraining strategies that incorporate additional domain data without catastrophic forgetting. Develop methods to retain general reasoning and language skills while enriching the model with vertical-specific knowledge. Enable the resulting MoE to serve as a foundation for the next stage of reasoning-driven or reinforcement-learning-based models across domains.  You’ll gain hands-on experience with state-of-the-art MoE adaptation strategies, domain-bridging continual pretraining, and evaluation of catastrophic forgetting. Strong contributors may also explore efficient expert routing strategies, language preservation methods, or extensions toward multi-modal adaptation.
 Prerequisite Knowledge:
 Strong self-motivation and research interest in LLM architectures and continual learning. Solid background in Transformers, MoE, and LLM pretraining/fine-tuning. Strong Python + PyTorch engineering skills (beyond coursework). (Bonus) Familiarity with continual learning challenges, e.g., catastrophic forgetting. (Bonus) Experience with large-scale distributed training or MoE routing strategies. (Bonus) Background in applying LLMs to domain-specific tasks (e.g., healthcare).
 Link(s) to Relevant Papers: 
 Zhou, Yuhang, et al. ""MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs."" NAACL (2025). Li, Hongbo, et al. ""Theory on Mixture-of-Experts in Continual Learning."" ICLR (2024). Jiang, Songtao, et al. ""Med-MoE: Mixture of Domain-Specific Experts for Lightweight Medical Vision-Language Models."" EMNLP (2024). Ke, Zixuan, et al. ""Achieving forgetting prevention and knowledge transfer in continual learning."" Advances in Neural Information Processing Systems 34 (2021).
 Project URLs:","08-27-2025","Adapting/Midtraining Multi-Modality Mixture-of-Experts (MoE) Foundation Models","Yes","MoE, Foundation Models, Midtraining, Domain Adaptation, Multi-Modality","On Campus","LTI","12-20+"
"","Project Contact:
 Maria Tagliaferri - mtagliaf@andrew.cmu.edu 
 Project Description:
 Join our team to help advance hip exoskeleton research by creating a real-time control interface with a simple, easy-to-use GUI. This interface will let researchers adjust controller settings live during experiments. Your work will make experiments faster and smoother, while giving you hands-on experience with Python, GUIs, and real-time systems in a cutting-edge robotics project
 Prerequisite Knowledge:
 Python, Basic understanding of embedded systems or single-board computers (e.g., Jetson, Raspberry Pi)
 Link(s) to Relevant Papers: 
 
 Project URLs:","08-27-2025","Hip Exoskeleton Controller Interface Development","Yes","Hip exoskeleton, GUI development, python","On Campus","Mechanical Engineering","5-10"
"","Project Contact:
 Chenyan Xiong - cx@andrew.cmu.edu 
 Project Description:
 AI models are revolutionizing healthcare and medicine—but can we build models that truly understand the complexities of medicine? This project focuses on building the next-generation healthcare foundation model by: Post-training strong open-source LLMs on large-scale EHR data to capture medical semantics. Developing a private Mixture-of-Experts (MoE) model for patient-specific EHR analysis and reasoning. Advancing core capabilities for agentic medical deep research, enabling robust, reasoning-driven healthcare applications.  You’ll gain hands-on experience with the full-stack LLM workflow for healthcare—from data curation and post-training to evaluation and system deployment. Strong students may also explore designing medical-specific evaluation suites, building efficient private inference pipelines, or extending foundation models to multi-modal patient data.
 Prerequisite Knowledge:
 Strong self-motivation and research interest in AI + healthcare. Comfort with Transformers and LLM training (post-training / fine-tuning). Solid Python + PyTorch engineering skills (beyond coursework). (Bonus) Familiarity with EHR data or clinical ontologies (ICD, CPT, etc.). (Bonus) Knowledge of Mixture-of-Experts models or distributed training. (Bonus) Experience with evaluation of reasoning or clinical prediction tasks.
 Link(s) to Relevant Papers: 
 Lee, Simon A., Anthony Wu, and Jeffrey N. Chiang. ""Clinical modernbert: An efficient and long context encoder for biomedical text."" arXiv preprint arXiv:2504.03964 (2025). An, Ulzee, et al. ""Dk-behrt: Teaching language models international classification of disease (icd) codes using known disease descriptions."" AAAI Bridge Program on AI for Medicine and Healthcare. PMLR, (2025). Su, Xiaorui, et al. ""Multimodal Medical Code Tokenizer."" Forty-second International Conference on Machine Learning (2025). Wornow, Michael, et al. ""Ehrshot: An ehr benchmark for few-shot evaluation of foundation models."" Advances in Neural Information Processing Systems 36 (2023). Lopez, Ivan, et al. ""Clinical entity augmented retrieval for clinical information extraction."" npj Digital Medicine 8.1 (2025). Pang, Chao, et al. ""CEHR-BERT: Incorporating temporal information from structured EHR data to improve prediction tasks."" Machine Learning for Health. PMLR (2021).
 Project URLs:","08-27-2025","Full-Stack Healthcare Foundation Model & Development","Yes","healthcare AI; MoE; Multimodality; LLM","On Campus","LTI","12+"
"","Project Contact:
 Chenyan Xiong - cx@andrew.cmu.edu 
 Project Description:
 Effective Long Sequence Native Sparse Attention for Agentic, Reasoning, and Recommendation Tasks. This project explores how to make LLMs truly useful on ultra-long inputs by replacing quadratic softmax attention with native sparse attention. You’ll help (1) benchmark open-source LLMs and sparse variants on nuanced long-sequence tasks—coding (SWE-bench), test-time scaled reasoning (e.g., long CoT/agent rollouts), and recommendation (ORBIT); (2) prototype post-training recipes that warm-start from full-attention checkpoints and learn sparsity patterns via long-sequence mid-training and task-specific data; and (3) analyze attention patterns to understand which signals matter beyond extractive QA/RAG. Strong students can also try designing new hardware-aligned sparsity patterns or training losses that encourage “native” sparsity.
 Prerequisite Knowledge:
 Strong self-motivation; interested in research. Comfort with Transformers/attention mechanism. Solid Python + Pytorch + Cpp experience, beyond coursework. (Bonus) CUDA kernels / performance profiling (Triton, nsys). (Bonus) Experience with evaluation benchmarks (SWE-bench, long-context suites)
 Link(s) to Relevant Papers: 
 Lee, Jinhyuk, et al. ""Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?."" arXiv preprint arXiv:2406.13121 (2024). Jiang, Zhengbao, et al. ""Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer."" arXiv preprint arXiv:2212.02027 (2022). Yuan, Jingyang, et al. ""Native sparse attention: Hardware-aligned and natively trainable sparse attention."" arXiv preprint arXiv:2502.11089 (2025). Yen, Howard, et al. ""Helmet: How to evaluate long-context language models effectively and thoroughly."" arXiv preprint arXiv:2410.02694 (2024). Hsieh, Cheng-Ping, et al. ""RULER: What's the Real Context Size of Your Long-Context Language Models?."" arXiv preprint arXiv:2404.06654 (2024). Lee, Jinhyuk, et al. ""LOFT: Scalable and More Realistic Long-Context Evaluation."" Findings of the Association for Computational Linguistics: NAACL 2025. 2025. Jimenez, Carlos E., et al. ""Swe-bench: Can language models resolve real-world github issues?."" arXiv preprint arXiv:2310.06770 (2023). Setlur, Amrith, et al. ""e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs."" arXiv preprint arXiv:2506.09026 (2025). He et al.  ""ORBIT - Open Recommendation Benchmark for Reproducible Research with Hidden Test"". Submission to NeurIPS 2025.
 Project URLs:","08-26-2025","Effective Long Sequence Native Sparse Attention for Agentic, Reasoning, and Recommendation Tasks.","Yes","MoE, Sparse Attention, Long-Context, Agents, Reasoning","On Campus","LTI","12-20+"
"","Project Contact:
 Conrad Borchers - cborcher@andrew.cmu.edu 
 Project Description:
 We are looking for a student to contribute to developing and testing web applications that leverage fine-grain learner performance data from tutoring systems. The application is designed to help students learn to set goals and stay motivated to achieve them.  This position is ideal for students looking to augment their portfolio or tap into research. Involvement in research activities, such as through UX tests and academic publications, is subject to the applicant's initiative and interest.
 Prerequisite Knowledge:
 Prior demonstrated experience in React.js, Vue.js or a similar framework is a strict requirement. When applying, please send your project GitHub link or source code with a README. Students with demonstrated experience with web application development projects, including backend development and Python web server development, will be preferred. UX testing experience is a plus but not required.
 Link(s) to Relevant Papers: 
 https://cborchers.com/pdf/Borchers-et-al-Goal-Dashboard.pdfhttps://kilthub.cmu.edu/articles/journal_contribution/Engagement_and_Learning_Benefits_of_Goal_Setting_with_Rewards_in_Human-AI_Tutoring/29825675/1/files/56902463.pdf
 Project URLs:","08-21-2025","Web Developer for AI-Supported Learning Goal Setting","Yes","vue.js, HTML+CSS+JS, research, edtech","Hybrid","HCII","10+"
"","Project Contact:
 Conrad Borchers - cborcher@andrew.cmu.edu 
 Project Description:
 We are seeking a motivated student with experience in R and/or Python to support research on learning in intelligent tutoring systems. The primary focus of this role is analyzing log data to compare student learning outcomes across different intelligent tutoring systems. This involves applying data processing, statistical analysis, and machine learning techniques to identify patterns in student interactions and learning gains. Potential research projects may include statistical modeling of the right amount of instructional support in tutoring systems, analyzing multimodal text and log data to predict learning outcomes, and studying collaborative learning in complex learning systems.  This position provides a unique opportunity to contribute to cutting-edge research on technology-enhanced learning while working with an interdisciplinary research team.  If you are interested in applying computational methods to education and gaining experience in data-driven learning science research, we encourage you to apply!
 Prerequisite Knowledge:
 R+Python experience with mulitmodal ML is a plus
 Link(s) to Relevant Papers: 
 https://arxiv.org/pdf/2504.05570https://arxiv.org/pdf/2506.17577https://osf.io/z4gru/download
 Project URLs:","08-21-2025","EduTech Data Science Researcher","Yes","R, Python, Multimodal, Stats, ML","Hybrid","HCII","10+"
"","Project Contact:
 Gavin Zhu - feiyuz@andrew.cmu.edu 
 Project Description:
 PartInstruct is a benchmark for fine-grained manipulation tasks with part-level instructions and diverse 3D object instances annotated with part-level labels. Solving it requires robust reasoning about object parts and their relationships with intended tasks. In this project, we plan to explore the state-of-the-art instruction following framework (e.g., language conditioned imitation learning, bi-level planning, etc.) for fine-grained tabletop manipulation and explore ways to make use of existing human knowledge to increase success rates.
 Prerequisite Knowledge:
 * Strong self-motivation * Interested in a research-oriented career * (Bonus) experience in manipulation (e.g., 16384, research projects, etc.) * (Bonus) experience with task and motion planning (e.g., 16350/16782) * (Bonus) experience with LLM/VLM agents * (Bonus) experience with embodied AI simulators/benchmarks (e.g., maniskills, meta world, etc.)
 Link(s) to Relevant Papers: 
 https://partinstruct.github.io
 Project URLs:","08-19-2025","Fine-grained Instruction Following in Manipulation Tasks","Yes","tabletop manipulation, bi-level planning, benchmark exploration","Hybrid","RI","15-20"
"","Project Contact:
 Gavin Zhu - feiyuz@andrew.cmu.edu 
 Project Description:
 Humans make use of both general knowledge (e.g., objects fall to the ground due to gravity) and specific demonstrations (e.g., observe an apple falling to the ground relatively quickly) to learn a world model efficiently. In our previous work, we showed that general knowledge can be used to instantiate a task-specific policy structure, which increases sample efficiency and robustness for robot policy learning. We would like to extend this work to learn not the policy but the world model for model-based reinforcement learning. In this project, you will develop a new model-based reinforcement learning framework that makes use of human domain knowledge such that it builds a sufficiently accurate model with very few interactions with the environment (online) or demonstrations (offline).
 Prerequisite Knowledge:
 * Strong self-motivation * Interested in a research-oriented career * Proficiency in Python and ML relevant packages such as PyTorch, NumPy, etc. * Previous experience in reinforcement learning (e.g., courses 10403/10703, 16831, Kaggle competition) * Some interactions with GPT or other LLMs * (Bonus) experience with LLM for code generation * (Bonus) experience with embodied AI simulators/benchmarks (e.g., gymnasium, maniskills, meta world, etc.)
 Link(s) to Relevant Papers: 
 https://arxiv.org/abs/2501.16546
 Project URLs:","08-19-2025","Knowledge-Informed Model-Based Reinforcement Learning","Yes","model-based reinforcement learning, LLM code synthesis, neuro-symbolic AI","Hybrid","RI","15-20"
"","Project Contact:
 Ren Butler - ddbutler@andrew.cmu.edu 
 Project Description:
 Computing instructors leverage team-based learning to develop students’ collaboration skills. However, team-based learning can feel psychologically unsafe when students have a variety of different cognitive and communicative processing styles. Therefore, we ask: what pedagogies and software tools can help computing students to feel psychologically safe in team-based learning?   We seek students with strong critical thinking, communication, and collaboration skills to participate in a research-through-design project. We will design a suite of practices and software to enhance intra-team communication and encourage psychological safety and learning behaviors in computing teams.  Student researchers will work in pairs through stages of the process, including identifying opportunities and challenges through cleaning, coding, and affinity diagramming interview data; understanding users through affinity diagramming and empathy maps; designing prototypes for conversational user interfaces and team health dashboards; and realizing these prototypes as web apps.   Our ultimate hope for this work is to celebrate all abilities in computing! We welcome students who are new to research.
 Prerequisite Knowledge:
 Required: Critical Thinking and Reading, Memo Writing,  Time Management, Communication, Collaboration  Helpful: Qualitative Coding, Affinity Diagramming, UX Design, Data Visualization, Full Stack Development with JavaScript or Python, Source Control, Discord
 Link(s) to Relevant Papers: 
 Fostering Psychological Safety for Interpersonal Learning in Neurodiverse Software Teams,http://darrendbutler.github.io/files/RESPECT_Doctoral_Consortium_Research_Summary.pdf  Preparing Autistic Students for the AI Workforce,http://darrendbutler.github.io/files/FSE_2025_PAS4AI.pdf
 Project URLs:
 https://cmu-variability.github.io/pas4aiworkforce/","08-18-2025","Fostering Psychological Safety in Team-Based Computing Education","Yes","collaborative learning, accessibility, computing education, design, web development","On Campus","HCII","15"
"","Project Contact:
 Steven Wu - zhiweiw@andrew.cmu.edu 
 Project Description:
 Our group is interested in the following types of questions: - How do we evaluate model capabilities and safety with little human oversight? - How do we align model behavior with multi-dimensional desiderata or human preferences?  - How do we prevent undesirable behavior such as reward hacking, memorization, and hallucination? Can we provide rigorous guarantees?
 Prerequisite Knowledge:
 (Advanced) machine learning; Probability; Linear Algebra; Experience in running large-scale ML experiments
 Link(s) to Relevant Papers: 
 -https://arxiv.org/abs/2401.04056 -https://arxiv.org/abs/2503.01067 -https://arxiv.org/abs/2406.13356 -https://arxiv.org/abs/2407.21057
 Project URLs:","08-16-2025","Reliable and Scalable Algorithms for AI Safety and Alignment","Yes","Safety, Alignment, Game Theory","On Campus","S3D","20"
"","Project Contact:
 Brian Railing - bpr@andrew.cmu.edu 
 Project Description:
 Ongoing statistical analysis and modeling of student behavior during computer-based exams
 Prerequisite Knowledge:
 15-213, stats (t-test, etc)
 Link(s) to Relevant Papers: 
 https://dl.acm.org/doi/10.1145/3641555.3705049,https://dl.acm.org/doi/10.1145/3478432.3499123
 Project URLs:","08-15-2025","Exam Analysis","Yes","examinations, statistics, systems","On Campus","CSD","10"
"","Project Contact:
 Reid Simmons - rsimmons@andrew.cmu.edu 
 Project Description:
 Tank, the roboceptionist, has been around for 20 years.  It is in need of a upgrade to modern technologies - speech and vision recognition, tight integration with LLMs, etc.  The project will involve updating one, or more, of these aspects to give Tank new capabilities and a fresh look.
 Prerequisite Knowledge:
 Experience with Python, C++, and Linux are required; Experience with speech recognition, computer vision, and LLMs useful, but not mandatory
 Link(s) to Relevant Papers: 
 http://www.cs.cmu.edu/~mmakatch/papers/believable_robots_aimag.pdf
 Project URLs:","08-15-2025","Roboceptionist","No","Human-Robot Interaction, LLMs, Speech, Vision","On Campus","RI","8-10"
"","Project Contact:
 Richard Border - rborder@andrew.cmu.edu 
 Project Description:
 Forward-time simulation allows us to learn about the relationship between genes and traits/disease across generations. The human genome can be efficiently represented as a special graph structure called a Genotype Representation Graph (GRG). GRGs dramatically reduce space complexity of large collections of genomes while enabling efficient linear algebraic operations. This project aims to incorporate additional dynamic biology features into GRGs (i.e., mating, recombination) to enable efficient GRG-based forward time simulation.
 Prerequisite Knowledge:
 Strong background in C++, familiarity with Python, courses covering dynamic programming and data structures. Basic understanding of genetics is helpful but not required.
 Link(s) to Relevant Papers: 
 https://www.nature.com/articles/s43588-024-00739-9 ## GRGshttps://doi.org/10.1101/2024.10.16.618755 ## forward time simulation
 Project URLs:","08-15-2025","High-performance forward-time simulation with graphical genome representations","Yes","computational biology, genetics, graphs, dynamic programming, high performance scientific computing","Hybrid","CBD","8-12 hours per week"
"","Project Contact:
 Yingtao Luo - yingtaol@andrew.cmu.edu 
 Project Description:
 We are a multidisciplinary student team, led by fourth-year PhD candidates specializing in medical informatics and large language models (LLMs), and advised by faculty members and practicing physicians. Our overarching mission is to advance agentic LLMs for healthcare, exploring applications such as medical copilots, deep medical research assistants, and medical reasoning models. More details will be shared with team members upon joining.
 Prerequisite Knowledge:
 Prerequisite Knowledge (Courses, Skills): 1. Fundamentals of Large Language Models (API usage, inference, core concepts) 2. Python programming 3. Basic statistical model or machine learning knowledge, including the full pipeline: data preparation, model training, evaluation, interpretation, and deployment considerations 4. Experience with Cursor or Claude Code, or equivalent coding tools  Preferred Additional Skills (any one of these): 1. Front-end development for web-based system deployment 2. Reinforcement learning for LLMs 3. LLM tool usage and development (e.g., MCP, retrieval over structured/unstructured sources) 4. Application of statistical or ML models to real-world problems, including end-to-end data handling (cleaning, processing) and deployment for domain-specific objectives (e.g., improving user experience, increasing revenue) 5. Clinical experience or connections in healthcare 6. Prior publications in AI or ML conferences
 Link(s) to Relevant Papers: 
 Multiple small papers marking intermediate achievements submitted already with the contributions from our great undergraduate student collaborators, such as to EMNLP and AAAI demo track, NeurIPS workshops. All of them will be expanded into main conference/journal papers. Our amazing undergrad students are co-authoring all these papers.
 Project URLs:
 Check this as an example:https://github.com/realYuanLi/meddr","08-13-2025","Agentic Medical AI System","Yes","Agentic LLM, AI for Health, Medical Decision-Making Support","Remote","MLD","10-20"
"","Project Contact:
 Alice Zhang - aqzhang@andrew.cmu.edu 
 Project Description:
 AI systems rely on massive amounts of human labor to ensure they're safe from content annotation that teaches models what's harmful to red teaming that stress-tests for vulnerabilities to safety evaluation that catches dangerous outputs. However, we have no clear map of where this critical AI safety work is happening, who's doing it, or how it's organized. Without understanding this ""human infrastructure of AI safety,"" researchers and policymakers can't effectively support these workers or improve AI safety practices. This project aims to develop a comprehensive taxonomy of organizations conducting AI safety labor. We'll investigate everything from crowdwork platforms handling content moderation to specialized firms doing AI red teaming, examining how different organizations structure safety tasks, recruit workers, and support well-being.  How to apply: Fill out this form:https://tinyurl.com/alice-ind-study-2025 and we will reach out if it's a good fit!
 Prerequisite Knowledge:
 Required: Strong prior academic performance as indicated through GPA and/or academic activities Basic programming experience (Python preferred) for web scraping and data processing  Preferred: Experience with web scraping tools or APIs Prior experience with qualitative research methods or content analysis Interest in AI safety, AI policy, or digital labor Ability to work independently and manage complex research projects
 Link(s) to Relevant Papers: 
 
 Project URLs:","08-13-2025","Who Keeps AI Safe? Mapping the Human Infrastructure Behind AI Safety","Yes","AI safety, qualitative research, web scraping","Hybrid","HCII","10-15"
"","Project Contact:
 Alice Zhang - aqzhang@andrew.cmu.edu 
 Project Description:
 AI red teaming, the practice of stress-testing AI systems by finding creative ways to break them, is crucial for AI safety. Red teamers need to think outside the box to uncover vulnerabilities that developers missed, but this creative work can be challenging and mentally taxing. How can we better support red teamers' creativity and help them discover novel risks in systems more effectively? This project uses co-design methods to develop creativity support tools specifically for AI red teaming. We'll run collaborative workshops with experienced red teamers to understand their creative processes, identify pain points, and iteratively design tools that enhance their ability to find system vulnerabilities and safety issues.  How to apply: Fill out this form:https://tinyurl.com/alice-ind-study-2025 and we will reach out if it's a good fit!
 Prerequisite Knowledge:
 Required: Strong academic performance as indicated through GPA or other academic work Strong communication and interpersonal skills for workshop facilitation Attention to detail Experience with qualitative research methods  Preferred: Experience with prototyping tools (Figma, Adobe Creative Suite, or similar) Background in HCI, design, psychology, or related fields Prior experience with user research methods or workshop facilitation Basic understanding of AI systems and AI red teaming
 Link(s) to Relevant Papers: 
 
 Project URLs:","08-13-2025","Designing Creativity Support Tools for AI Red Teaming","Yes","creativity support, AI safety, AI evaluation","Hybrid","HCII","10-15"
"","Project Contact:
 Alice Zhang - aqzhang@andrew.cmu.edu 
 Project Description:
 Who ensures AI products are safe? Responsible AI content workers spend time stress testing systems and annotating problematic content to ensure product safety. However, research on supporting their well-being is limited—some workers aren't even warned about harmful content they'll encounter. This project investigates effective ways to warn workers through controlled experiments, comparing responses across disclosure conditions from no warning to detailed harm categorization with examples.  How to apply: Fill out this form:https://tinyurl.com/alice-ind-study-2025 and we will reach out if it’s a good fit!
 Prerequisite Knowledge:
 Required:  Strong prior academic performance as indicated through GPA and/or academic activities Basic data processing experience (e.g., statistical analysis through R or Python)  Preferred:  Prior experience with literature review (e.g., in coursework or previous research) Understanding of experimental design concepts Strong attention to detail and ability to collaborate in a larger group setting Preference will be given to students interested in further research in the broader topic of AI safety and worker well-being
 Link(s) to Relevant Papers: 
 
 Project URLs:","08-13-2025","Mind the Gap: How Risk Disclosure Affects Digital Workers' Performance","Yes","Experiment, mental health, AI safety","Hybrid","HCII","10-15"
"","Project Contact:
 Dominik Moritz - dmoritz2@andrew.cmu.edu 
 Project Description:
 Mosaic is an extensible architecture for linking data visualizations, tables, input widgets, and other data-driven components. It's widely used to build scalable visualization tools likehttps://github.com/apple/embedding-atlas for AI datasets. In this project, we are developing a Python API for Mosaic similar to the popular Altair package (which I help develop) for visualizations on top of Vega-Lite.
 Prerequisite Knowledge:
 Python, JavaScript, Git
 Link(s) to Relevant Papers: 
 https://idl.uw.edu/papers/mosaic,https://idl.uw.edu/papers/altair
 Project URLs:
 https://idl.uw.edu/mosaic/","08-12-2025","Python API for Mosaic","Yes","visualization, data science, open source","Hybrid","HCII","10"
"","Project Contact:
 Christine Kwon - ckwon2@andrew.cmu.edu 
 Project Description:
 This project investigates how students collaborate in computer-supported collaborative learning (CSCL) environments.  We are also exploring the use of Large Language Models (LLMs) to automatically annotate collaborative moments, paving the way for scalable, AI-assisted analysis of teamwork in CSCL.
 Prerequisite Knowledge:
 Strong programming skills to replicate, extend, and optimize the LLM pipeline
 Link(s) to Relevant Papers: 
 
 Project URLs:","08-12-2025","Detecting Collaborative Moments in Computer-Supported Collaborative Learning","Yes","computer-supported collaborative learning, technology-enhanced learning, Large Language Models (LLMs), and learning sciences","Hybrid","HCII","5-20"
"","Project Contact:
 Alicia Lee - hlee3@andrew.cmu.edu 
 Project Description:
 The project is a reflective sleep-aid game that helps players externalize and transform self-critical thoughts through symbolic rituals and conversational AI.
 Prerequisite Knowledge:
 I'm especially looking for RAs who are interested in one of the roles: - LLM-based prototyping - Developing responsive physical Prototyping (e.g., object responds to voice or proximity) - Game or playful experience design
 Link(s) to Relevant Papers: 
 (Accepted at CHI PLAY Companion '25) ""I Can’t Turn My Brain Off: Exploring Conversational AI for Self-Critical Thoughts in VR""https://doi.org/10.1145/3744736.3749350.
 Project URLs:","08-12-2025","Thoughts Trader: game that transform your thoughts/emotions/dreams","Yes","LLM, game, reflective gameplay, digital intervention for well-being, conversational AI","On Campus","HCII","9 or 12 credits"
"","Project Contact:
 Alicia Lee - hlee3@andrew.cmu.edu 
 Project Description:
 This project extends the collaboration between HCII and Robotics, focusing on older adults with Mild Cognitive Impairment (MCI). The aim is to develop and evaluate a resilient care coordination system that adapts to uncertainties, changes in goals, care network structures, and the care context. The study involves the iterative development of a testbed care coordination system, followed by a field study. The follow-up study will focus on a deployment study. The system aims to reduce the burden of managing care schedules, maintain strong social ties, alleviate recipients' feelings of being a burden, and enhance overall care quality.
 Prerequisite Knowledge:
 [Technical RA] Background: Data analysis, Python programming Tasks: - Work with nested JSON files (e.g., Google Location History) - Process and analyze time-series and geospatial data  [User Research RA] Background: HCI, UX research, or design Tasks: - Review transcripts, notes, and observation data - Participate in affinity diagramming and synthesis sessions
 Link(s) to Relevant Papers: 
 Unremarkable to Remarkable AI Agent: Exploring Boundaries of Agent Intervention for Adults With and Without Cognitive Impairmenthttps://dl.acm.org/doi/abs/10.1145/3711098  Dynamic Agent Affiliation: Who Should the AI Agent Work for in the Older Adult's Care Network?https://dl.acm.org/doi/abs/10.1145/3643834.3661500
 Project URLs:
 https://ai-caring.org/","08-12-2025","AI-Caring: Piggybacking & Resilient Care","Yes","AI agent, agent, AI design, dementia, HCI","On Campus","HCII","9 or 12 credits"
"","Project Contact:
 Quang Dao - qvd@andrew.cmu.edu 
 Project Description:
 Every time you send a secure message or make an online purchase, you are trusting cryptographic protocols to protect your data. But how do we know these protocols actually work? Traditional pen-and-paper proofs can contain subtle logical gaps, and real-world implementations often introduce additional vulnerabilities.  Machine-checkable proofs solve this verification problem. By encoding both the cryptographic protocol and its security proof in a theorem prover like Lean, we can guarantee the correctness of security arguments; every logical deduction is verified by the computer, eliminating potential errors.  This project will use Lean to formalize standard cryptographic building blocks, often learned in an introductory cryptography course.  What you'll gain: - Understanding of cryptographic fundamentals - Skills translating crypto concepts into formal Lean models - Experience constructing machine-verified security proofs - Contributions to a verified cryptography library  Prior experience with proof assistants or cryptography strongly preferred.
 Prerequisite Knowledge:
 Required: - Comfort in understanding and writing formal mathematical proofs  Preferred: - Knowledge of abstract algebra, discrete mathematics, or theoretical computer science - Basic knowledge of cryptographic primitives and security definitions - Familiarity with proof assistants, particularly Lean
 Link(s) to Relevant Papers: 
 https://lean-lang.org/https://joyofcryptography.com/https://eprint.iacr.org/2024/1819.pdf
 Project URLs:
 https://github.com/dtumad/VCV-io","08-12-2025","Lean into Verified Cryptography","Yes","cryptography, formalization, lean, formal verification","Hybrid","CSD","8"
"","Project Contact:
 Chi-En Teh - cteh@andrew.cmu.edu 
 Project Description:
 In disaster zones, every second can mean the difference between life and death. We’re developing the first system capable of accurately measuring heart rate outdoors using cameras, this could let robots rapidly assess victims’ conditions in mass casualty events. By uniting medicine, AI, robotics, physics, and computer vision, we’re bringing critical, contact-free vital sign monitoring to the field, where it’s needed most.
 Prerequisite Knowledge:
 Proficiency in Python for algorithm development and prototyping Strong background in Computer Vision and Machine Learning Experience with signal processing techniques, including time-series analysis, filtering, and spectral analysis Version control and collaborative coding practices using Git/GitHub Performance benchmarking and statistical analysis Nice to have:  Background in ROS2   Familiarity with camera systems (RGB, NIR, multispectral, or thermal) and calibration methods Understanding of real-time processing and optimization for edge devices (e.g., NVIDIA Jetson)
 Link(s) to Relevant Papers: 
 https://biomedical-engineering-online.biomedcentral.com/articles/10.1186/s12938-025-01405-5 https://dl.acm.org/doi/pdf/10.1145/3558518
 Project URLs:
 https://triagechallenge.darpa.mil/","08-11-2025","Contactless Heart Rate in Outdoor Environments","Yes","Heart Rate through Video","Hybrid","RI","10"
"","Project Contact:
 Howie Choset - choset@andrew.cmu.edu 
 Project Description:
 The Biorobotics Lab reduces complicated high-dimensional problems found in robotics to low-dimensional simpler ones for design, analysis, and planning. Often, we look to biology for inspiration and sometimes, we return the favor by providing analysis that models biology.
 Prerequisite Knowledge:
 Programming in Python or C.
 Link(s) to Relevant Papers: 
 Research and Education Agenda for Howie Choset, Biorobotics Lab, Carnegie Mellon.  Intro / Snake Robots. Howie Choset has led a comprehensive research program in bio-inspired robotics and AI since he started as a faculty member at Carnegie Mellon in 1996. Perhaps the work for which Professor Choset is best known are his group’s snake robots. This work has garnered much media attention, such as being the first robot to appear on Late Night TV, as well as appearing on Good Morning America, back in 1998. CMU has asked Choset to speak on several occasions about the snake robots, including CMU’s first appearance at the World Economic Forum in 2011 and many appearances on the Hill. The notoriety is only the tip of the iceberg. Choset’s true contributions lie in the fundamental science his group pioneered, as well as, in the systems, his staff and students, designed, built, and deployed in real-world settings. Choset’s group runs the pipeline from basic research, to deployment and sometimes to commercialization, which in turn informs the basic research.    Geometric Mechanics. The core challenge Professor Choset’s group faces lies in coordinating the motion of all of the joints of the mechanism to produce purposeful motion. This is especially challenging for snake robots because they have many more joints, often called degrees of freedom, than conventional robots. To tackle this challenge, Choset’s group takes recourse to fundamental principles in a branch of mathematics called geometric mechanics. Geometric mechanics is essentially a combination of differential geometry, group theory and vector calculus. We certainly are not the first to investigate Geometric Mechanics, and it was brought to us from researchers at Caltech, where Professor Choset obtained his PhD. Our contribution to Geometric Mechanics can succinctly be stated as bringing a robotics kinematics perspective to the field, allowing us to advance the field so as to solve both low and high degree of freedom problems that generate motions for snake robots, and other systems, to locomote.   Professor Choset’s students, Drs. Elie Shammas (American University of Beruit), Ross Hatton (Oregon State University), Chaohui Gong (Bito Robotics) and Tony Dear (Columbia University) all have made or continue to make contributions to this field. It is worth highlighting Ross Hatton’s work which was able to address fundamental questions posed by a Nobel Laureate, and then answered by Professor Hatton in his PhD work. Dr. Hatton continues to advance the field of Geometric mechanics, and is considered to be the thought leader in this field.  Professors Hatton and Choset are co-authoring a book on this subject to be used in their classes at their respective universities. Much of this book is Professor Hatton’s framework on Geometric Mechanics which makes the field highly accessible to new Engineering and Computer Science students. It will be the must-have book for robot locomotion, once it is published.   Bio-inspired Robotics / Robot-inspired Biology. Many researchers investigate biological systems in hopes of finding clues or motivations on how to build and program robotic systems that operate in challenging terrains. Choset’s group differs from these so-called biologically inspired robotics research groups in that his group seeks to use the geometric methods, described above, to model biological behavior and then once understood, try to replicate the behavior on a robot. However, something more provocative emerged: since Choset’s group took fundamental recourse to understand the mechanics of what makes the robot move, his group has made contributions to understanding biological locomotion; in other words, Choset’s group pursued a reverse illumination on using robots to understand biology. As such, his group, working in close collaboration with Professor Daniel Goldman at Georgia Tech, has advanced the fundamental math to model the motion of sand swimming lizards, sidewinders, and mudskipper fish. Baxi Chong, an MS student with Professor Choset, who continued with Professor Goldman for his PhD, also continues this work as a Professor at Penn State. Currently, Professor Choset is working with Professor Scott Kelly at the University of North Carolina to use the geometric methods to model how fish and other organisms communicate in swarms.  Search and Rescue. The concepts developed in Professor Choset’s group have also been implemented on real robot systems. This is important because experiments and deployments stress test the core assumptions, many times inadvertently, that were made when deriving new frameworks for robotics. Moreover, we all want to have societal impact with our work, and professor Choset’s group certainly has.  Professor Choset’s group also deployed their locomoting snake robots in real search and rescue scenarios, including the immediate aftermath of the Mexico City Earthquake, where his research group, at the invitation of the Mexican Red Cross, cleared buildings with their robots. Virtually all of Professor Choset’s field deployments and theoretical grounding in Search and Rescue comes from Professor Robin Murphy, formerly of Texas A&M and now CMU. Her guidance in connecting theory and mechanism to reality has been priceless. Our experiences in breaking the snake robots in the field drove us to develop modular systems, described below.   Archeology. Professor Choset’s group also sent robots into caves, off the coast of the Red Sea and underneath the pyramids in Egypt to locate archeological artifacts; we found nothing. One of the challenges we faced in Egypt was enabling the snake robot to climb up sandy hills, a challenge we overcame when we started working with Professor Goldman’s group at Georgia Tech. This demonstrates how real-world deployment shines light onto problems otherwise overlooked.  Underwater Robotics. Professor Choset’s group built an underwater snake robot that has been deployed in swimming pools, the Allegheny River and a port in the Pacific Ocean. In addition, working with JPL, Choset’s team helped develop another underwater snake robot, to be deployed on one of the moons of Saturn. This moon has a very large underground ocean into which the robot would be sent, with the hopes of finding signs of life. While this program was indeed canceled, Choset’s team did deploy a robot in an ice skating rink in Pasadena and a glacier in Canada.   Medical Robotics. Professor Choset and is students developed a small surgical snake robot whose success led Professor Choset to co-found, along with his post-doc Dr. Alon Wolf (Technion) and medical collaborator Dr. Marco Zenati, MD (Harvard)  a medical robotics company called Medrobotics. Medrobotics snake robot was the first medical robot to clear the FDA with the indication “robot.” We operated on over 1500 people in 4 continents and 17 countries. Among our largest investors were the Petersen Family, local to Pittsburgh and Western Pennsylvania.   Body-SLAM. Real-world deployment of the snake robots taught us a lot. The first lesson is simply answering the question: where am I? A considerable amount of effort has been dedicated to addressing the localization problem in robotics. Choset’s group differs from the others in that we develop localization algorithms (and sensors) for confined spaces, for applications specifically dedicated to medicine, search and rescue and manufacturing. Professor Choset’s students Drs. Stephen Tully (Activ Surgical) and Arun Srinivatsan (Apple) developed novel filtering methods that tolerate large initial uncertainty and yet converge to the correct estimate quite quickly. Dr Srivtastan’s work, in particular, incorporated the use of dual quaternions and Bingham filters that allowed us to prescribe a linear filter in rotational space, as opposed to linearizing on such spaces. Today, Dr. Srivatsan is one of the research leaders at Apple.    Medical AI. Ultrasound is another sensing modality that Professor Choset’s group, along with Professors Artur Dubrawski and John Galleotti at CMU, has been using to close the loop with medical diagnostics and therapy. Ultrasound is low cost, portable and emits no radiation, and yet is difficult to interpret and requires mechanical interaction between the patient’s anatomy and caregiver. Professor Choset’s group developed a novel needle insertion mechanism that uses both force feedback and ultrasound to guide the needle into a vessel. The guidance system is a machine learning model that identifies the vessel and directs the robot to aim the needle toward such vessel. Unfortunately, as with most machine learning approaches, this approach requires data, which is virtually non-existent because labeled ultrasound data does not exist in the wild. Instead, Professors Galleotti’s and Choset’s students developed a novel synthetic data generation technique that provides data of vessels being compressed at a large range of pressures, thereby allowing the model to be trained and the needle to be inserted. This approach has been validated on many live pig models at the University of Pittsburgh.  Modularity. The second lesson we learnt from deploying the snake robots lies with modularity. Already, Professor Choset’s thinking had been inspired by Professor Mark Yim at the University of Pennsylvania and Choset’s early snake designs were based on Yim’s. Modularity became important as a result of Professor Choset’s group cycle of: build, deploy, break, re-pair. Instead of re-building a new system, Professor Choset’s students Matt Tesch (Hebi Robotics) and David Rollinson (Hebi Robotics) decided to create modules, both in hardware and software, that enabled Choset’s team to just repair a module, and not the entire system. By developing and committing to interfaces among modules, Choset’s group was able to accelerate the development, repair and maintenance of novel robots because one need only design a module. This architecture led Choset, along with Rollsinson, Tesch, and two others Layton and Enner, to co-found Hebi Robotics to commercialize this technology. Hebi has customers and employees from all over the world, now based in Pittsburgh.   The work in modularity also runs the pipeline as well. Dr. Whitman (Boston Dynamics) along with MS students Mr. Jeff Hu developed novel machine learning models to address the problem of modular robot design. Their work addressed: given a set of modules, generate the optimal controller and arrangement of modules to execute a locomotion task. Naturally, we demonstrated the ideas of Dr. Whitman’s work on the Hebi modular robot system, as well as a new modular robot architecture, called Eigenbot, a project led by Li in Choset’s group.   Multi-Agent Planning. Just as snake and modular robots have many degrees of freedom, so do multi-agent systems and swarms, which also possess the challenge of coordinating all of the degrees of freedom to provide purposeful motion. Professor Choset’s students Drs. Glenn Wager and Richard Ren developed a novel multi-agent planning paradigm that, on average, beats the so-called curse of dimensionality, where two robots are twice as hard as one, three robots are four times more difficult, four robots are eight times, and so forth. Dr. Ren went on to address a large variety of multi-agent planners, including multi-objective multi-agent planning. Currently, he is writing a textbook on this topic, which will be the go-to text for all aspiring students in multi-agent planning. Recently, Mr. Anoop Bhat, Choset’s current PhD students, in cooperation with Professor Siva Ramithian at Texas A&M, have been developing all sorts of varieties of multi-agent traveling salesman problems, with the added complication that the targets to be visited by the agents move. This will have several applications in the Operations Research community.   Information-based Search. Professor Choset’s research group also investigates multi-agent search based on information that is either known previously or acquired at run-time. Choset’s graduate student, Ms. Ananyal Rao, co-advised with Professor  David Wettergreen, along with Choset’s post-doc, Dr. Bhaskar Vundurthy, are advancing the field of ergodic search, where agents use an a priori information map to guide the team of agents to balance the time searching for targets in regions based on the probability of finding a target in that region. This approach addresses the so-called exploration versus exploitation challenge commonly found in most optimization-based methods. Choset’s work differs from prior work in that it considers heterogeneous agents, and can search for multiple objectives at once. Another distinguishing feature of our work is that it uses classic geometric methods, used in Choset’s PhD thesis.   Manufacturing in Confined Spaces. Professor Choset’s group also developed large and strong snake robots for manufacturing in confined spaces with Boeing. Just as we developed mechanisms and algorithms for confined spaces, we also develop sensors with edge computing in confined spaces. The Boeing project initiated Professor Choset’s group’s work to develop such sensing capabilities. This project, led by Mr. Lu Li (Pipe Force / CMU) in Professor Choset’s group, uses conventional sensing modality, such as structured light, combined with novel custom developed electronics and software to enable point cloud data to be generated in tight spaces where conventional range sensors would not operate. Recently, Mr. Li, along with Ms. Tian (Pipe Force), an MS student in Choset’s group, co-founded a company called Pipe Force, AI, whose mission is to provide high resolution geo-synchronized digital twin maps of underground pipes. These maps will then be used to assess the state of such underground infrastructure allowing repairs to be planned proactively as opposed to reactively which is time consuming and expensive. Already, we have deployed our system in Texas and will begin mapping the underground storm pipes at CMU in the fall.   Decentralized Manufacturing / Space Robotics. As with our other projects, Choset’s students and staff built a group of multi-agent modular mobile bases for decentralized multi-robot assembly of aircraft wings for Boeing. This decentralized planning work is now being advanced to enable construction of large structures in space. One of the physical challenges for on-orbit assembly comes down to putting a peg in a hole. This requires careful coordination of the robot “feel” its way into a hole. To prescribe such algorithms, we developed a “holodeck” system that has four robot arms situated on rails. The combined motion of the robots on the rails mimics motion of objects in space on which we can test our controllers for peg-in-hole operations. This work is being done in cooperation with Northrup-Grummun.   Additive / Edge Manufacturing. Professor Choset’s research group has been collaborating with aerospace manufacturers such as Boeing, Northrop-Grumman and Raytheon to pioneer robotic and AI technology that enables additive and intelligent manufacturing. Their work, which has garnered several best paper awards at prestigious robotics and automation conferences such as ICRA and IROS, focuses on two key innovations. The first is edge sensor-based planning, which uses a custom-designed, close-range 3D and tactile sensor to acquire multimodal information, such as color, shape, force and contact, for in-situ inspection and monitoring. This allows for real-time motion adjustments and adaptive path replanning, ensuring precision and quality. The second innovation is the use of AI for intelligent in-process additive repair workflows. This approach utilizes edge sensor information to characterize material properties, enabling closed-loop process control, to reduce manufacturing defects and material waste. The group is currently working with industry partners to deploy these technologies on next-generation airplane and engine production lines, to promote a safer and eco-friendly future transportation.  Painting and Coating. Many applications require the deposition of spray paints or coatings. Most often, the goal is to achieve complete, uniform coverage of a precise thickness. This quality can be challenging to achieve even in simple planar surfaces, let alone complex curved surfaces or within confined spaces. This is important in the automotive industry where conventional approaches require three to five months to program a robot to “cover” a car. We have completed our work with Ford, and now Professor Choset’s group works with Boeing to program ink-jet applicators to create beautiful designs while painting, really ink-jetting, aircraft.   Abstraction. In many of the above described projects, a common theme emerges: reducing complicated problems to simpler ones so as analysis, design, and planning can take place in a tractable and practical manner. Starting with Choset’s PhD Thesis on deformation retractions with Voronoi Diagrams, hsi work reduced high-dimensional searchers to one-dimensional searchers. Likewise, the above-mentioned multi-agent planners search in one-dimensional spaces, only graduating to higher dimensional ones, as needed.  This abstraction theme permeates the machine learning work of Mr. Ben Freed, whose ultimate goal is that of forming simplified representations of complex problems that lend themselves to planning and decision-making.  Consider the problem of driving to the store to buy a loaf of bread.  This task requires the completion of many precise motor commands, completed in the correct sequence, making planning difficult.  However, given a set of skills (e.g., a driving skill, a walking skill, and an object manipulation skill), the planning problem can be greatly simplified by planning over skills instead of raw motor commands.  Mr. Freed, along with his co-advisor Jeff Schneider, is developing methods for automatically extracting such useful task representations. Here, we try to learn the low-level skills and the high-level policy to combine them, as a byproduct of the model learning procedure so that the skills are naturally aligned with the model of the dynamics and environment. The task of planning becomes far simpler if one can ignore the low-level details and therefore planning is abstracted and simplified.     Education. One of the cornerstones of Professor Choset’s work is making the fundamental math, probability and algorithms easily accessible - so much so, that newcomers to these fields underestimate the contributions because they seem “so easy.” His presentations tend to be informative, educational, and often entertaining. This has carried over to Professor Choset’s classes, where he developed and continually taught undergraduates for nearly 20 years. Professor Choset created the Robotics Minor in 1998 and the Second Major in Robotics in 2007. According to the letters that supported Professor Choset’s nomination for the coveted Doherty Award, undergraduate education at Carnegie Mellon is a result of Professor Choset’s dedication and hard work toward undergraduate education. Choset’s dream aspiration is to package the educational experiences so that they can be used at Universities, other than Carnegie Mellon, with the hopes of impacting community college education.  Outreach. Finally, Professor Choset gives presentations to young people in K-12 schools, and on occasion to the elderly. In 2025, Choset hosted a refugee from Afghanistan who was part of the all-girl robotics team, called the Dreamers, which was shut down when the Taliban regained control. The girls and their families had to escape Afganastan. In 2023, Angel Studios produced a movie called Rule Breakers which was a dramatization of how this team formed. Two of Professor Choset’s students, Ananya Rao and Yizhu Gu, and one of his robots, Peggy, appeared in this movie. Both Ms. Rao and Gu now have IMb pages. This movie celebrates not only girls in STEM, but overcoming hardships to pursue one’s dreams, which in this case was robotics. In exchange for the students and robot, the producers let Professor Choset hold a screening in Pittsburgh where members of the CMU, Pitt, and CCAS communities were invited for a free preview of the movie. Professor Choset’s student from the Dreamers gave a powerful speech, and then Ms. Patti Rote spoke about the All-Girls team, Girls of Steel, that she, along with Professor George Kantor, started and mentored for many years. It was an incredible evening.
 Project URLs:
 http://biorobotics.org","08-11-2025","Biorobotics Lab","Yes","Bio-inspired Robots, Multi-agent planning, Search, Abstraction, etc","On Campus","RI","10"
"","Project Contact:
 Justin Chan - justinc3@andrew.cmu.edu 
 Project Description:
 We are working on a wireless sticker sensor using NFC tags (the same technology used in ApplePay), that you can stick onto any object like a water bottle or a pipe and track the amount of fluid present. This technology requires no batteries, and you can a reel of these stickers at a cost of cents per sticker. Imagine a reel of sensor which you can take an instrument any vessel with fluid and easily read with your smartphone, no special reader needed.
 Prerequisite Knowledge:
 Machine Learning, Signal Processing using Matlab or Python, Arduino/Raspberry Pi implementations
 Link(s) to Relevant Papers: 
 
 Project URLs:","08-11-2025","Wireless sticker for measuring moisture levels","Yes","wireless,sticker,liquid","On Campus","S3D","12"
"","Project Contact:
 Justin Chan - justinc3@andrew.cmu.edu 
 Project Description:
 This project will develop (1) a contactless radar based system to sense miniscule muscle motions and tremors associated with fatigue and Parkinson's disease, (2) an e-skin sticker on the skin that can amplify the tiny motions and work at long range to make the system practical.
 Prerequisite Knowledge:
 Machine Learning, Signal Processing using Matlab or Python, Arduino/Raspberry Pi implementations
 Link(s) to Relevant Papers: 
 
 Project URLs:","08-11-2025","GigaFlex: AI-driven Contactless sensing of muscle motions and tremors for Parkinson's disease","Yes","muscle,ai,radar","On Campus","S3D","12"
"","Project Contact:
 Justin Chan - justinc3@andrew.cmu.edu 
 Project Description:
 Ever wondered if microphones could record sounds so soft that humans cannot even hear them? As it turns out, recording sub-audible sounds is something that happens everyday as part of a test to screen for every new baby born in the US. It turns out that human ears *produce* extremely soft sounds like a speaker that are an indicator of hearing at different frequencies. In this project we will use a low-cost earphone connected to a smartphone to detect these sounds in our sounds and make it work robustly in different conditions.
 Prerequisite Knowledge:
 Machine Learning, Signal Processing using Matlab or Python, Arduino/Raspberry Pi implementations
 Link(s) to Relevant Papers: 
 
 Project URLs:","08-11-2025","SubSonicSounds: Measuring sub-audible sounds using smartphones","Yes","sounds,smartphones,acoustics","On Campus","S3D","12"
"","Project Contact:
 Justin Chan - justinc3@andrew.cmu.edu 
 Project Description:
 Imagine navigating a sophisticated user interface on smart glasses while on the go using only your eyes and attention, without the use of tiring or awkward hand gestures and voice commands.  SSVEPs (steady-state visual evoked potentials) are a robust brain-computer interaction that can enable screen interaction using only visual attention and minimal user training. They have been successfully used in high-precision applications including virtual keyboards and robotic control with 97-99% accuracy.   When a user focuses on a flickering target, the visual cortex automatically generates detectable brain waves (EEG) at the same frequency, allowing real-time, hands-free selection.  However, conventional SSVEP systems use flicker frequencies that can cause substantial visual fatigue.  Here, we propose an imperceptible SSVEP-based BCI tailored for extended reality (XR) devices like smart glasses and headsets that leverage AI-generated flicker patterns that is imperceptible to the eye.
 Prerequisite Knowledge:
 Machine Learning, Signal Processing using Matlab or Python, Arduino/Raspberry Pi implementations
 Link(s) to Relevant Papers: 
 
 Project URLs:","08-11-2025","Neural Spectre: AI/ML-Driven Imperceptible Brain Computer Interfaces for Extended Reality","Yes","bci,ssvep,ai,ml,hci","On Campus","S3D","12"
"","Project Contact:
 Chase Norman - chasen@andrew.cmu.edu 
 Project Description:
 Terence Tao's first YouTube video prominently features my automated theorem prover, Canonical:https://youtu.be/cyyR7j2ChCI?t=1911  Canonical is a Rust program that exhaustively searches for terms of a given type in dependent type theory. This can be used to prove theorems, synthesize programs, and find counterexamples.  We have projects for every specialization. We're developing programming languages, training artificial intelligences, writing high-performance software, and devising new formal methods techniques. Feel free to reach out.
 Prerequisite Knowledge:
 Genuine enthusiasm is the only requirement. 15-321 helps for Lean-focused projects 15-312 helps for PL-focused projects
 Link(s) to Relevant Papers: 
 https://arxiv.org/abs/2504.06239
 Project URLs:
 https://chasenorman.com","08-11-2025","Canonical","Yes","Automated Theorem Proving, Formal Methods, Programming Languages","Hybrid","CSD","8"
"","Project Contact:
 David Lindlbauer - dlindlba@andrew.cmu.edu 
 Project Description:
 Virtual Reality (VR) offers full control over space, time, and body representation, opening new opportunities for capturing and replaying user actions. This project explores how body duplication - the ability to record and replay one's own body movements - can be leveraged to support productivity and self-guidance in VR environments. Imagine leaving behind a ghost of yourself performing a task such as assembling an object, sorting items, or navigating a complex space, and later reviewing or learning from it. These duplications of the self could help users monitor progress, repeat physical workflows, or plan actions more effectively across time.  In this work, the student will design and implement a VR prototype that enables:  Recording of body poses and movements over time (e.g., using headset and hand tracking, or full MOCAP system); Replay of body duplicates in VR, aligned in time and space; Interaction with the duplicate through controls like play, pause, and rewind, enabling the user to follow or learn from their past actions;  The student will conduct a user study to investigate how body duplication affects user performance, recall, task planning, and subjective perception of presence and control, in different use cases such as repetitive workflows, training routines, etc.  [Benefits] Explore cutting-edge concepts in temporal interaction, embodiment, and productivity in VR; Gain experience in Human-Computer Interaction research, particularly user-centered design and evaluation; Gain hands-on experience with body tracking, animation playback, and spatial design; Opportunity for top-tier academic publications (e.g., CHI, UIST); depending on the student’s commitment and contributions, they may co-author a research paper.
 Prerequisite Knowledge:
 Interest in embodiment and interaction in immersive systems; Strong programming skills (in C#, Python, etc); Experience with development environments for AR/VR (such as Unity, Unreal, Godot, etc); Familiarity with animation and body tracking is a plus; Experience with study design and statistical analysis are a plus; Need to be self-motivated and have a proactive attitude.
 Link(s) to Relevant Papers: 
 
 Project URLs:
 https://augmented-perception.org/","08-11-2025","Exploring Body Duplication in VR for Productivity and Self-Guidance","No","Virtual Reality, Collaboration","On Campus","HCII","10-15"
"","Project Contact:
 David Lindlbauer - dlindlba@andrew.cmu.edu 
 Project Description:
 Virtual Reality (VR) has gained traction in recent years, as it offers the potential to provide immersive and interactive experiences that are not possible with traditional user interfaces. With a variety of interaction methods available, including controller-based and hand-tracking approaches, VR has the potential to facilitate more personalized and adaptive user experiences. That said, how can we leverage the emotional states of users to drive adaptive interactions? By understanding the role of emotions in shaping user experience, we can create virtual environments that are more engaging, intuitive, and responsive to individual needs. In this work, the student will design and implement different interaction styles that either match or counteract users’ emotional state and conduct a user study where different users will experience these different interactions.  As an example, when experiencing anger, instead of a simple touch button activated with the hand (neutral), an example of such an interaction style could be: (Match anger) - the button activates only if pressed with some power strength or via repeated interactions mimicking, for instance, a punching bag; (Counteract anger) - the button activates only if gently pressed or unlocks only after the user has taken a couple breaths in and out;  A user study will compare the effectiveness of these interaction styles in improving task performance and user satisfaction. Participants will be exposed to both matching and counteracting interactions in different scenarios. The study will gather data on task completion times, as well as subjective feedback regarding user experience and preference for each style. In sum, the goal of this project is to provide insights into how emotionally adaptive interfaces should be designed to best support users in different tasks (i.e., a set of design guidelines).
 Prerequisite Knowledge:
 Interest and aptitude for user interface design; Strong programming skills (in C#, Python, etc); Experience with development environments for AR/VR (such as Unity, Unreal, Godot, etc); Experience with study design and statistical analysis are a plus; Need to be self-motivated and have a proactive attitude;
 Link(s) to Relevant Papers: 
 
 Project URLs:
 https://augmented-perception.org/","08-11-2025","Emotion-Driven Interactions: Designing Adaptive XR Interactions for Matching and Counteracting User Emotions","No","Extended Reality, Adaptive User Interfaces","Hybrid","HCII","10-15"
"","Project Contact:
 David Lindlbauer - dlindlba@andrew.cmu.edu 
 Project Description:
 This project explores how virtual objects in extended reality (XR) environments can move more naturally and expressively—like elements in a living ecosystem. Rather than staying rigid or snapping into fixed positions, UI elements in this system will float apart to avoid collisions, drift into elegant formations, or respond fluidly to flick and pull gestures. These behaviors aim to create a more immersive, low-effort, and visually expressive interaction experience. We’ll prototype an XR interface where motion is driven by a lightweight physics system and potentially augmented with learned or rule-based flow behaviors. An automatic layout controller will help maintain legibility and spatial order as objects respond dynamically to user input and environmental context. You will work closely with a PhD student to help bring this system to life—either by implementing physics-driven motion behaviors, building the gesture interaction layer, or designing the layout management logic. Depending on your interests and skills, you might prototype dynamic UI components in Unity, experiment with movement smoothing and constraint logic, or create demo scenes that showcase expressive motion in XR. The technical details and system architecture will evolve as the project develops, but your contributions will be core to how this novel interaction style is realized and evaluated.  What you’ll learn Hands-on experience in designing and prototyping expressive, interactive motion in XR interfaces; or integrating physics systems or gesture input into dynamic UI components. Research exposure with the potential to co-author a paper at premiere HCI/graphics venues (e.g., CHI, UIST, SIGGRAPH).
 Prerequisite Knowledge:
 Strong programming skills (Python, C#/C++, or Swift) Interest in dynamic interaction, fluid UI, or XR prototyping Experience with XR development frameworks (Unity, Unreal, WebXR, ARCore, or ARKit) is a plus. Experience with implementing computer graphics, visual effects (VFX) or physics-based or shader-driven animation is a plus. Self-motivated and able to commit **at least 10 hours per week** working in the lab during the project period.
 Link(s) to Relevant Papers: 
 Agarawala, A. and Balakrishnan, R. Keepin’ It Real: Pushing the Desktop Metaphor with Physics, Piles and the Pen. CHI 2006. [video] Bell, B. and Feiner, S. Dynamic Space Management for User Interfaces. UIST 2000. Wilson A., et al. Bringing Physics to the Surface. UIST 2008.
 Project URLs:
 https://augmented-perception.org/","08-11-2025","Organically Moving XR Interfaces","No","Extended Reality, Computational Interaction","On Campus","HCII","10-15"
"","Project Contact:
 David Lindlbauer - dlindlba@andrew.cmu.edu 
 Project Description:
 This project explores how augmented reality (AR) glasses can support people during real-world search tasks—like finding a car in a crowded parking lot, navigating a confusing indoor environment, or locating something based on distant sound cues. In such situations, users rely on incomplete and often ambiguous audio-visual information.  Our goal is to build an assistant that recognizes when the user is confused, inefficient, or deviating from optimal behavior—and offers subtle, timely guidance through visual or auditory feedback. To do this, we simulate what optimal search strategies might look like in a given environment (e.g., using ideal observer models or expert behaviors), and compare them to how the user is actually behaving.  You will work closely with a PhD student to build the simulation platform or behavioral data pipeline that supports this research. This could involve creating a simple AR-style search task (e.g., using Unity or WebXR) or writing scripts to process user data such as gaze and head direction, audio localization, or movement. Your contributions will help us run experiments and test how assistive systems should respond to different user behaviors.
 Prerequisite Knowledge:
 Strong programming skills (C#/C++, Python, or Swift) Experience with Unity, WebXR, or 3D interaction frameworks is a plus. Interest in XR, human behavior modeling, or assistive technologies Self-motivated and able to commit at least 10 hours per week working in the lab during the project period.
 Link(s) to Relevant Papers: 
 Murray-Smith, R., et al. What Simulation Can Do for HCI Research. ACM Interactions 2022. Gebhardt, C., et al. Learning Cooperative Personalized Policies from Gaze Data. UIST 2019.
 Project URLs:
 https://augmented-perception.org/","08-11-2025","Audio-Visual Search Assistant on AR Glasses","No","Augmented Reality, Smart Glasses","On Campus","HCII","10-15"
"","Project Contact:
 David Lindlbauer - dlindlba@andrew.cmu.edu 
 Project Description:
 Computational agents are improving quickly. They demonstrate increasingly powerful and realistic human behaviors. They can even, to some extent, model specific people. We anticipate that within our lifetime, it will become common for people to create personalized agents: systems that not only reflect their knowledge and preferences but also interact with others on their behalf. This will fundamentally transform the way we communicate.  This project aims to provide a glimpse of this future by building a functional prototype of a personalized conversation agent on Slack. The system will have to build a representation of how a person responds to different social contexts based on their prior conversation history, then continue conversations as the individual they are modeled after.  The project will result in a working system and early insight into the opportunities and challenges of deploying personalized conversational agents, contributing to future designs of computer-mediated communication platforms.  Benefits Hands-on experience integrating LLMs into a real-world communication platform Potential to contribute to publishable research in top-tier venues (e.g., CHI, UIST).
 Prerequisite Knowledge:
 Strong programming skills and the ability to build working prototypes Proficiency in backend development (Node.js) Familiarity with large language models (e.g., OpenAI API) prototyping Interest in HCI
 Link(s) to Relevant Papers: 
 Conversational Agents on Your Behalf: Opportunities and Challenges of Shared Autonomy in Voice Communication for Multitasking. Cheng et al. CHI 2025.  Generative Ghosts: Anticipating Benefits and Risks of AI Afterlives. Morris and Brubaker. CHI 2025. AI-Mediated Communication: Definition, Research Agenda, and Ethical Considerations. Hancock et al. Journal of Computer-Mediated Communication 2020.  Dittos: Personalized, Embodied Agents That Participate in Meetings When You Are Unavailable. Leong et al. CSCW 2024.  Social-RAG: Retrieving from Group Interactions to Socially Ground AI Generation. Wang et al. CHI 2025.
 Project URLs:
 https://augmented-perception.org/","08-11-2025","Slack Surrogates: Deploying Personalized Conversational Agents In-the-Wild","No","Conversational Agents, Generative User Interfaces, Computational Interaction","On Campus","HCII","10-15"
"","Project Contact:
 David Lindlbauer - dlindlba@andrew.cmu.edu 
 Project Description:
 User interfaces for Augmented Reality (AR) are currently unpleasant to interact with. In particular, their limited field of view, resolution, and input make interactions with traditional interfaces cumbersome. For instance, consider browsing the New York Times with a small 70-degree view window. Because the display limits the amount the user can see at once, they either have to zoom out of the window, which makes the text more difficult to read, or slowly scan through parts of the window bit by bit. Both cases can quickly result in eye and neck strain.   In this project, we investigate the feasibility of automatically adapting interfaces for AR leveraging generative models. To accommodate the smaller field of view, we propose decomposing an interface into its constituent elements and only showing to the user a relevant subset. To determine how exactly we should decompose an interface into its constituent elements and what to show in the final AR interface, we propose leveraging existing knowledge on user flows (i.e., a user's journey to complete specific tasks within an application or service).   This project will result in a collection of AR interface designs and a working system, contributing to knowledge on how future interfaces should be designed for glasses-based form factors.  Required Skills (1) strong programming skills (Python, JavaScript, C#, or Swift preferred), or  (2) strong UX background and experience in designing interface prototypes (e.g., Figma) Familiarity with large language models (e.g., OpenAI API) prototyping is a plus  Benefits Hands-on experience building and designing AI-augmented UIs. Potential to contribute to publishable research in top-tier venues (e.g., CHI, UIST).
 Prerequisite Knowledge:
 Required Skills (1) strong programming skills (Python, JavaScript, C#, or Swift preferred), or  (2) strong UX background and experience in designing interface prototypes (e.g., Figma)  Familiarity with large language models (e.g., OpenAI API) prototyping is a plus
 Link(s) to Relevant Papers: 
 Mobbin.https://mobbin.com/ Generative and Malleable User Interfaces with Generative and Evolving Task-Driven Data Model. Cao et al. CHI 2025. Automatic Macro Mining from Interaction Traces at Scale. Huang et al. CHI 2024.
 Project URLs:
 https://augmented-perception.org/","08-11-2025","Generating Augmented Reality Interfaces Leveraging User Flows","No","Augmented Reality, Computational Interaction, Generative User Interfaces","On Campus","HCII","10-15"
"","Project Contact:
 Zhiqiu Lin - zhiqiul@andrew.cmu.edu 
 Project Description:
 We build large-scale, richly annotated cinematic video-language datasets, retrieval-augmented creative tools, and interactive visual creation interfaces—paired with scalable human oversight and reward modeling—to make controllable, Hollywood-grade video generation accessible to researchers, professionals, and everyday creators.
 Prerequisite Knowledge:
 We’re looking for candidates with at least one of the following:  1 - Experience building multimodal chatbot agents.  2 - Familiarity with visual creation tools such as Midjourney, Runway, or Lovart.  3 - Strong coding skills for developing interactive frontends and scalable backends.  4 - Experience in RLHF, video foundation models, or human–computer interaction.
 Link(s) to Relevant Papers: 
 
 Project URLs:","08-11-2025","Agentic Interface for Video Generation","Yes","Video Generation, Video Understanding, Human-Computer Interaction, Agent","Hybrid","RI","20"
"","Project Contact:
 Yuyu Lin - yuyulin@andrew.cmu.edu 
 Project Description:
 An unpowered hand exoskeleton will be lightweight, conformal to fingers, and thus support broad interaction scenarios. This project combines mechanical engineering, biomechanics, human factors, and interaction design to create an adaptable and adjustable device that can augment natural hand movements for various applications, including virtual reality, rehabilitation, and assistive technology.
 Prerequisite Knowledge:
 tangible prototyping (incl. modelling & 3d printing)
 Link(s) to Relevant Papers: 
 This project will build on previous work:https://interactive-structures.org/publications/2025-09-bistable-orthosis/
 Project URLs:","08-11-2025","Unpowered Reconfigurable Hand Exoskeleton for Interactions","Yes","Bistable Structures, VR haptics, Hand Exoskeleton","On Campus","HCII","9-12"
"","Project Contact:
 Min Xu - mxu1@andrew.cmu.edu 
 Project Description:
 We have a number of exploratory projects on developing AI techniques for the automated analysis of biomedical images. More information about our research can be found on our lab website. http://cs.cmu.edu/~mxu1  Following are our recent publications and software projects as exampleshttps://scholar.google.com/citations?hl=en&user=Y3Cqt0cAAAAJ&view_op=list_works&sortby=pubdate https://github.com/xulabs/aitom  If you are interested, it would be nice if you could fill up the following questionnaire.https://forms.gle/Z4zFutVjutoYfTPE8
 Prerequisite Knowledge:
 Python Programming
 Link(s) to Relevant Papers: 
 
 Project URLs:
 http://cs.cmu.edu/~mxu1","08-11-2025","Biomedical image analysis","Yes","Biomedical image analysis; Computer Vision; Machine Learning;","Hybrid","CBD","5"

